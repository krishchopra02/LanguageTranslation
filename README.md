# LanguageTranslation

A language translation model built on the Transformer based Encoder Decoder archietecture from the famous "Attenion is all you need" paper. https://arxiv.org/abs/1706.03762?context=cs

The model is built in TensorFlow and achieved a loss of 0.944 (masked Sparse Categorical Cross Entropy loss) and a BLEU score of 31.8

For more details: https://cloud.google.com/translate/automl/docs/evaluate
https://cwiki.apache.org/confluence/display/MXNET/Multi-hot+Sparse+Categorical+Cross-entropy

